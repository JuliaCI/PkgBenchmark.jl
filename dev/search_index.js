var documenterSearchIndex = {"docs":
[{"location":"run_benchmarks/#Running-a-benchmark-suite-1","page":"Running a benchmark suite","title":"Running a benchmark suite","text":"","category":"section"},{"location":"run_benchmarks/#","page":"Running a benchmark suite","title":"Running a benchmark suite","text":"DocTestSetup  = quote\n    using PkgBenchmark\nend","category":"page"},{"location":"run_benchmarks/#","page":"Running a benchmark suite","title":"Running a benchmark suite","text":"Use benchmarkpkg to run benchmarks defined in a suite as defined in the previous section.","category":"page"},{"location":"run_benchmarks/#","page":"Running a benchmark suite","title":"Running a benchmark suite","text":"benchmarkpkg","category":"page"},{"location":"run_benchmarks/#PkgBenchmark.benchmarkpkg","page":"Running a benchmark suite","title":"PkgBenchmark.benchmarkpkg","text":"benchmarkpkg(pkg, [target]::Union{String, BenchmarkConfig}; kwargs...)\n\nRun a benchmark on the package pkg using the BenchmarkConfig or git identifier target. Examples of git identifiers are commit shas, branch names, or e.g. \"HEAD~1\". Return a BenchmarkResults.\n\nThe argument pkg can be a name of a package or a path to a directory to a package.\n\nKeyword arguments:\n\nscript - The script with the benchmarks, if not given, defaults to benchmark/benchmarks.jl in the package folder.\npostprocess - A function to post-process results. Will be passed the BenchmarkGroup, which it can modify, or return a new one.\nresultfile - If set, saves the output to resultfile\nretune - Force a re-tune, saving the new tuning to the tune file.\nverbose::Bool = true - Print currently running benchmark.\nlogger_factory - Specify the logger used during benchmark.  It is a callable object (typically a type) with no argument that creates a logger.  It must exist as a constant in some package (e.g., an anonymous function does not work).\nprogressoptions - Deprecated.\n\nThe result can be used by functions such as judge. If you choose to, you can save the results manually using writeresults where results is the return value of this function. It can be read back with readresults.\n\nExample invocations:\n\nusing PkgBenchmark\n\nimport MyPkg\nbenchmarkpkg(pathof(MyPkg)) # run the benchmarks at the current state of the repository\nbenchmarkpkg(pathof(MyPkg), \"my-feature\") # run the benchmarks for a particular branch/commit/tag\nbenchmarkpkg(pathof(MyPkg), \"my-feature\"; script=\"/home/me/mycustombenchmark.jl\")\nbenchmarkpkg(pathof(MyPkg), BenchmarkConfig(id = \"my-feature\",\n                                            env = Dict(\"JULIA_NUM_THREADS\" => 4),\n                                            juliacmd = `julia -O3`))\nbenchmarkpkg(pathof(MyPkg),  # Run the benchmarks and divide the (median of) results by 1000\n    postprocess=(results)->(results[\"g\"] = median(results[\"g\"])/1_000)\n\n\n\n\n\n","category":"function"},{"location":"run_benchmarks/#","page":"Running a benchmark suite","title":"Running a benchmark suite","text":"The results of a benchmark is returned as a BenchmarkResult:","category":"page"},{"location":"run_benchmarks/#","page":"Running a benchmark suite","title":"Running a benchmark suite","text":"PkgBenchmark.BenchmarkResults","category":"page"},{"location":"run_benchmarks/#PkgBenchmark.BenchmarkResults","page":"Running a benchmark suite","title":"PkgBenchmark.BenchmarkResults","text":"Stores the results from running the benchmarks on a package.\n\nThe following (unexported) methods are defined on a BenchmarkResults (written below as results):\n\nname(results)::String - The commit of the package benchmarked\ncommit(results)::String - The commit of the package benchmarked. If the package repository was dirty, the string \"dirty\" is returned.\njuliacommit(results)::String - The commit of the Julia executable that ran the benchmarks\nbenchmarkgroup(results)::BenchmarkGroup - a BenchmarkGroup  contaning the results of the benchmark.\ndate(results)::DateTime - The time when the benchmarks were executed\nbenchmarkconfig(results)::BenchmarkConfig - The BenchmarkConfig used for the benchmarks.\n\nBenchmarkResults can be exported to markdown using the function export_markdown.\n\n\n\n\n\n","category":"type"},{"location":"run_benchmarks/#More-advanced-customization-1","page":"Running a benchmark suite","title":"More advanced customization","text":"","category":"section"},{"location":"run_benchmarks/#","page":"Running a benchmark suite","title":"Running a benchmark suite","text":"Instead of passing a commit, branch etc. as a String to benchmarkpkg, a BenchmarkConfig can be passed","category":"page"},{"location":"run_benchmarks/#","page":"Running a benchmark suite","title":"Running a benchmark suite","text":"PkgBenchmark.BenchmarkConfig","category":"page"},{"location":"run_benchmarks/#PkgBenchmark.BenchmarkConfig","page":"Running a benchmark suite","title":"PkgBenchmark.BenchmarkConfig","text":"BenchmarkConfig(;id::Union{String, Nothing} = nothing,\n                 juliacmd::Cmd = `joinpath(Sys.BINDIR, Base.julia_exename())`,\n                 env::Dict{String, Any} = Dict{String, Any}())\n\nA BenchmarkConfig contains the configuration for the benchmarks to be executed by benchmarkpkg.\n\nThis includes the following:\n\nThe commit of the package the benchmarks are run on.\nWhat julia command should be run, i.e. the path to the Julia executable and\n\nthe command flags used (e.g. optimization level with -O).\n\nCustom environment variables (e.g. JULIA_NUM_THREADS).\n\nThe constructor takes the following keyword arguments:\n\nid - A git identifier like a commit, branch, tag, \"HEAD\", \"HEAD~1\" etc.        If id == nothing then benchmark will be done on the current state        of the repo (even if it is dirty).\njuliacmd - Used to execute the benchmarks, defaults to the julia executable              that the Pkgbenchmark-functions are called from. Can also include command flags.\nenv - Contains custom environment variables that will be active when the         benchmarks are run.\n\nExamples\n\njulia> using Pkgbenchmark\n\njulia> BenchmarkConfig(id = \"performance_improvements\",\n                       juliacmd = `julia -O3`,\n                       env = Dict(\"JULIA_NUM_THREADS\" => 4))\nBenchmarkConfig:\n    id: performance_improvements\n    juliacmd: `julia -O3`\n    env: JULIA_NUM_THREADS => 4\n\n\n\n\n\n","category":"type"},{"location":"run_benchmarks/#","page":"Running a benchmark suite","title":"Running a benchmark suite","text":"This object contains the package commit, julia command, and what environment variables will be used when benchmarking. The default values can be seen by using the default constructor","category":"page"},{"location":"run_benchmarks/#","page":"Running a benchmark suite","title":"Running a benchmark suite","text":"julia> BenchmarkConfig()\nBenchmarkConfig:\n    id: nothing\n    juliacmd: `/home/user/julia/julia`\n    env:","category":"page"},{"location":"run_benchmarks/#","page":"Running a benchmark suite","title":"Running a benchmark suite","text":"The id is a commit, branch etc as described in the previous section. An id with value nothing means that the current state of the package will be benchmarked. The default value of juliacmd is joinpath(Sys.BINDIR, Base.julia_exename() which is the command to run the julia executable without any command line arguments.","category":"page"},{"location":"run_benchmarks/#","page":"Running a benchmark suite","title":"Running a benchmark suite","text":"To instead benchmark the branch PR, using the julia command julia -O3 with the environment variable JULIA_NUM_THREADS set to 4, the config would be created as","category":"page"},{"location":"run_benchmarks/#","page":"Running a benchmark suite","title":"Running a benchmark suite","text":"julia> config = BenchmarkConfig(id = \"PR\",\n                                juliacmd = `julia -O3`,\n                                env = Dict(\"JULIA_NUM_THREADS\" => 4))\nBenchmarkConfig:\n    id: \"PR\"\n    juliacmd: `julia -O3`\n    env: JULIA_NUM_THREADS => 4","category":"page"},{"location":"run_benchmarks/#","page":"Running a benchmark suite","title":"Running a benchmark suite","text":"To benchmark the package with the config, call benchmarkpkg as e.g.","category":"page"},{"location":"run_benchmarks/#","page":"Running a benchmark suite","title":"Running a benchmark suite","text":"benchmark(\"Tensors\", config)","category":"page"},{"location":"run_benchmarks/#","page":"Running a benchmark suite","title":"Running a benchmark suite","text":"info: Info\nThe id keyword to the BenchmarkConfig does not have to be a branch, it can be most things that git can understand, for example a commit id or a tag.","category":"page"},{"location":"run_benchmarks/#","page":"Running a benchmark suite","title":"Running a benchmark suite","text":"Benchmarks can be saved and read using writeresults and `readresults respectively:","category":"page"},{"location":"run_benchmarks/#","page":"Running a benchmark suite","title":"Running a benchmark suite","text":"PkgBenchmark.readresults\nPkgBenchmark.writeresults","category":"page"},{"location":"run_benchmarks/#PkgBenchmark.readresults","page":"Running a benchmark suite","title":"PkgBenchmark.readresults","text":"readresults(file::String)\n\nReads the BenchmarkResults stored in file (given as a path).\n\n\n\n\n\n","category":"function"},{"location":"run_benchmarks/#PkgBenchmark.writeresults","page":"Running a benchmark suite","title":"PkgBenchmark.writeresults","text":"writeresults(file::String, results::BenchmarkResults)\n\nWrites the BenchmarkResults to file.\n\n\n\n\n\n","category":"function"},{"location":"define_benchmarks/#Defining-a-benchmark-suite-1","page":"Defining a benchmark suite","title":"Defining a benchmark suite","text":"","category":"section"},{"location":"define_benchmarks/#","page":"Defining a benchmark suite","title":"Defining a benchmark suite","text":"Benchmarks are to be written in <PKGROOT>/benchmark/benchmarks.jl and are defined using the standard dictionary based interface from BenchmarkTools, as documented here. The naming convention that must be used is to name the benchmark suite variable SUITE. An example file using the dictionary based interface can be found here. Note that there is no need to have PkgBenchmark loaded to define the benchmark suite.","category":"page"},{"location":"define_benchmarks/#","page":"Defining a benchmark suite","title":"Defining a benchmark suite","text":"note: Note\nRunning this script directly does not actually run the benchmarks, this is the job of PkgBenchmark, see the next section.","category":"page"},{"location":"comparing_commits/#Comparing-commits-1","page":"Comparing commits","title":"Comparing commits","text":"","category":"section"},{"location":"comparing_commits/#","page":"Comparing commits","title":"Comparing commits","text":"You can use judge to compare benchmark results of two versions of the package.","category":"page"},{"location":"comparing_commits/#","page":"Comparing commits","title":"Comparing commits","text":"PkgBenchmark.judge","category":"page"},{"location":"comparing_commits/#BenchmarkTools.judge","page":"Comparing commits","title":"BenchmarkTools.judge","text":"judge(pkg::String,\n      [target]::Union{String, BenchmarkConfig},\n      baseline::Union{String, BenchmarkConfig};\n      kwargs...)\n\nArguments:\n\npkg - The path to the package to benchmark, use pathof(Package)\ntarget - What do judge, given as a git id or a BenchmarkConfig. If skipped, use the current state of the package repo.\nbaseline - The commit / BenchmarkConfig to compare target against.\n\nKeyword arguments:\n\nf - Estimator function to use in the judging.\njudgekwargs::Dict{Symbol, Any} - keyword arguments to pass to the judge function in BenchmarkTools\n\nThe remaining keyword arguments are passed to benchmarkpkg\n\nReturn value:\n\nReturns a BenchmarkJudgement\n\n\n\n\n\njudge(target::BenchmarkResults, baseline::BenchmarkResults, f;\n      judgekwargs = Dict())\n\nJudges the two BenchmarkResults in target and baseline using the function f.\n\nReturn value\n\nReturns a BenchmarkJudgement\n\n\n\n\n\n","category":"function"},{"location":"comparing_commits/#","page":"Comparing commits","title":"Comparing commits","text":"which returns a BenchmarkJudgement","category":"page"},{"location":"comparing_commits/#","page":"Comparing commits","title":"Comparing commits","text":"PkgBenchmark.BenchmarkJudgement","category":"page"},{"location":"comparing_commits/#PkgBenchmark.BenchmarkJudgement","page":"Comparing commits","title":"PkgBenchmark.BenchmarkJudgement","text":"Stores the results from running a judgement, see judge.\n\nThe following (unexported) methods are defined on a BenchmarkJudgement (written below as judgement):\n\ntarget_result(judgement)::BenchmarkResults - the BenchmarkResults of the target.\nbaseline_result(judgement)::BenchmarkResults -  the BenchmarkResults of the baseline.\nbenchmarkgroup(judgement)::BenchmarkGroup - a BenchmarkGroup  containing the estimated results\n\nA BenchmarkJudgement can be exported to markdown using the function export_markdown.\n\nSee also BenchmarkResults\n\n\n\n\n\n","category":"type"},{"location":"#PkgBenchmarks-1","page":"Home","title":"PkgBenchmarks","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"PkgBenchmark provides an interface for Julia package developers to track performance changes of their packages.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"The package contains the following features","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Running the benchmark suite at a specified commit, branch or tag. The path to the julia executable, the command line flags, and the environment variables can be customized.\nComparing performance of a package between different package commits, branches or tags.\nExporting results to markdown for benchmarks and comparisons, similar to how Nanosoldier reports results for the benchmarks in Base Julia.","category":"page"},{"location":"#Installation-1","page":"Home","title":"Installation","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"PkgBenchmark is registered so installation is done by running import Pkg; Pkg.add(\"PkgBenchmark\").","category":"page"},{"location":"export_markdown/#Export-to-markdown-1","page":"Export to markdown","title":"Export to markdown","text":"","category":"section"},{"location":"export_markdown/#","page":"Export to markdown","title":"Export to markdown","text":"It is possible to export results from PkgBenchmark.BenchmarkResults and  PkgBenchmark.BenchmarkJudgement  using the function export_markdown","category":"page"},{"location":"export_markdown/#","page":"Export to markdown","title":"Export to markdown","text":"export_markdown","category":"page"},{"location":"export_markdown/#PkgBenchmark.export_markdown","page":"Export to markdown","title":"PkgBenchmark.export_markdown","text":"export_markdown(file::String, results::BenchmarkResults)\nexport_markdown(io::IO,       results::BenchmarkResults)\nexport_markdown(file::String, results::BenchmarkJudgement; export_invariants=false)\nexport_markdown(io::IO,       results::BenchmarkJudgement; export_invariants=false)\n\nWrites the results to file or io in markdown format.\n\nWhen exporting a BenchmarkJudgement, by default only the results corresponding to possible regressions or improvements will be included. To also export the invariant results, set export_invariants=true.\n\nSee also: BenchmarkResults, BenchmarkJudgement\n\n\n\n\n\n","category":"function"},{"location":"export_markdown/#Using-Github.jl-to-upload-the-markdown-to-a-Gist-1","page":"Export to markdown","title":"Using Github.jl to upload the markdown to a Gist","text":"","category":"section"},{"location":"export_markdown/#","page":"Export to markdown","title":"Export to markdown","text":"Assuming that we have gotten a BenchmarkResults or BenchmarkJudgement from a benchmark, we can then use GitHub.jl to programmatically upload the exported markdown to a gist:","category":"page"},{"location":"export_markdown/#","page":"Export to markdown","title":"Export to markdown","text":"julia> using GitHub, JSON, PkgBenchmark\n\njulia> results = benchmarkpkg(\"PkgBenchmark\");\n\njulia> gist_json = JSON.parse(\n            \"\"\"\n            {\n            \"description\": \"A benchmark for PkgBenchmark\",\n            \"public\": false,\n            \"files\": {\n                \"benchmark.md\": {\n                \"content\": \"$(escape_string(sprint(export_markdown, results)))\"\n                }\n            }\n            }\n            \"\"\"\n        )\n\njulia> posted_gist = create_gist(params = gist_json);\n\njulia> url = get(posted_gist.html_url)\nURI(https://gist.github.com/317378b4fcf2fb4c5585b104c3b177a8)","category":"page"},{"location":"export_markdown/#","page":"Export to markdown","title":"Export to markdown","text":"note: Note\nConsider using an extension to your browser to make the gist webpage use full width in order for the tables in the gist to render better, see e.g here.","category":"page"}]
}
